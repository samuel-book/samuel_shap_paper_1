{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Describing model predictions, using SHAP values and SHAP interactions\n",
    "\n",
    "## Plain English summary\n",
    "\n",
    "When fitting a machine learning model to data to make a prediction, it is now possible, with the use of the SHAP library, to allocate contributions of the prediction onto the feature values. This means that we can now turn these black box methods into transparent models and describe what the model used to obtain it's prediction.\n",
    "\n",
    "SHAP values are calculated for each feature of each instance for a fitted model. In addition there is the SHAP base value which is the same value for all of the instances. The base value represents the models best guess for any instance without any extra knowledge about the instance (this can also be thought of as the \"expected value\"). It is possible to obtain the models prediction of an instance by taking the sum of the SHAP base value and each of the SHAP values for the features. This allows the prediction from a model to be transparant, and we can rank the features by their importance in determining the prediction for each instance.\n",
    "\n",
    "In our previous notebooks (03_xgb_combined_shap_key_features.ipynb) we saw that a feature with the same value in multiple instances (such as all of the patients that attend hospital A), the feature (Hospital A) does not necessarily have the same SHAP value in all of those instances. This means that the feature value alone is not a clear indication of the impact it will have on the prediction - this is due to there being feature interactions, such that SHAP values for a feature are influenced by the other feature values. This means that SHAP values are instance dependent (as they are also capturing the interactions between pairs of feature values). The SHAP values therefore are in turn made up of a main effect (what is due to the feature value, the standalone effect) and also the interactions with the other features (a value per feature pairings).\n",
    "\n",
    "[Note: In this notebook we will refer to the parts of the SHAP value consistently as base value, main effect, and interactions, where the term SHAP feature value refers to the sum of the main effect and interactions].\n",
    "\n",
    "SHAP values are in the same units as the model output (for XGBoost these are in log odds). \n",
    "\n",
    "Here we fit a light GBM model to the SAMueL dataset, to predict whether a patient recieves thrombolysis from the values of 8 features (in this notebook we have removed the hospital ID as this would need to be one-hot encoded and would increase the number of pairwise feature SHAP iinterations beyond what is computationally possible). We calculate the SHAP values (base, main effect and feature interactions) of this fitted model and show the most useful way (that we have found) to present all of these values in order to gain the most insight into how the model is working. At present this is using a grid of SHAP dependency plots.\n",
    "\n",
    "This notebook is based on the blog https://towardsdatascience.com/analysing-interactions-with-shap-8c4a2bc11c2a\n",
    "\n",
    "## Model and data\n",
    "\n",
    "XGBoost model was trained on all of the data (no test set used). The 7 features in the model are:\n",
    "\n",
    "* Arrival-to-scan time: Time from arrival at hospital to scan (mins)\n",
    "* Infarction: Stroke type (1 = infarction, 0 = haemorrhage)\n",
    "* Stroke severity: Stroke severity (NIHSS) on arrival\n",
    "* Precise onset time: Onset time type (1 = precise, 0 = best estimate)\n",
    "* Prior disability level: Disability level (modified Rankin Scale) before stroke\n",
    "* Use of AF anticoagulents: Use of atrial fibrillation anticoagulant (0 = No, 1 = Yes)\n",
    "* Onset-to-arrival time: Time from onset of stroke to arrival at hospital (mins)\n",
    "* Stroke team: Represented as integer-encoded categorical features (see note below)\n",
    "\n",
    "And one target feature:\n",
    "* Thrombolysis: Did the patient recieve thrombolysis (0 = No, 1 = Yes)\n",
    "\n",
    "## Aims\n",
    "\n",
    "* Fit Light GBM model using feature data to predict whether patient gets thrombolysis\n",
    "* Calculate the SHAP main effect and SHAP interaction values\n",
    "* Understand the SHAP main effect and SHAP interaction values\n",
    "* Find the best way to display these values in order to gain the most insight into the relationships that the model is using\n",
    "\n",
    "## Observations\n",
    "\n",
    "* SHAP interactions are awesome! \n",
    "* Viewing them as a grid of SHAP dependency plots clearly shows the overall relationships that the model uses to derive it's predictions for the whole dataset.\n",
    "\n",
    "Note: From https://lightgbm.readthedocs.io/en/latest/Features.html#optimal-split-for-categorical-features: It is common to represent categorical features with one-hot encoding, but this approach is suboptimal for tree learners. Particularly for high-cardinality categorical features, a tree built on one-hot features tends to be unbalanced and needs to grow very deep to achieve good accuracy. Instead of one-hot encoding, the optimal solution is to split on a categorical feature by partitioning its categories into 2 subsets. If the feature has k categories, there are 2^(k-1) - 1 possible partitions. But there is an efficient solution for regression trees. It needs about O(k * log(k)) to find the optimal partition. The basic idea is to sort the categories according to the training objective at each split. More specifically, LightGBM sorts the histogram (for a categorical feature) according to its accumulated values (sum_gradient / sum_hessian) and then finds the best split on the sorted histogram.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kerry/anaconda3/envs/samuel2/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import machine learning methods\n",
    "#import lightgbm\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Import shap for shapley values\n",
    "import shap # `pip install shap` if neeed\n",
    "\n",
    "# Turn warnings off to keep notebook tidy\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create output folders if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './saved_models'\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in JSON file\n",
    "\n",
    "Contains a dictionary for plain English feature names for the 8 features selected in the model. Use these as the column titles in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./output/feature_name_dict.json\") as json_file:\n",
    "    feature_name_dict = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "Data has previously been split into 5 stratified k-fold splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loc = '../data/kfold_5fold/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise empty lists\n",
    "train_data, test_data = [], []\n",
    "\n",
    "# Read in the names of the selected features for the model\n",
    "number_of_features_to_use = 8\n",
    "key_features = pd.read_csv('./output/feature_selection.csv')\n",
    "key_features = list(key_features['feature'])[:number_of_features_to_use]\n",
    "# And add the target feature name: S2Thrombolysis\n",
    "key_features.append('S2Thrombolysis')\n",
    "\n",
    "# For each k-fold split\n",
    "for i in range(5):\n",
    "    # Read in training set, restrict to chosen features, rename titles, & store\n",
    "    train = pd.read_csv(data_loc + 'train_{0}.csv'.format(i))\n",
    "    train = train[key_features]\n",
    "    train.rename(columns=feature_name_dict, inplace=True)\n",
    "    train_data.append(train)\n",
    "    # Read in test set, restrict to chosen features, rename titles, & store\n",
    "    test = pd.read_csv(data_loc + 'test_{0}.csv'.format(i))\n",
    "    test = test[key_features]\n",
    "    test.rename(columns=feature_name_dict, inplace=True)\n",
    "    test_data.append(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise, train the model using all the data. Join up all of the test data (by definition, each instance exists only once across all of the 5 test sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform categorical features into the appropriate type that is expected by LightGBM.\n",
    "From https://www.kaggle.com/code/mlisovyi/beware-of-categorical-features-in-lgbm/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 88792 entries, 0 to 17757\n",
      "Data columns (total 9 columns):\n",
      " #   Column                    Non-Null Count  Dtype   \n",
      "---  ------                    --------------  -----   \n",
      " 0   Arrival-to-scan time      88792 non-null  float64 \n",
      " 1   Infarction                88792 non-null  int64   \n",
      " 2   Stroke severity           88792 non-null  float64 \n",
      " 3   Precise onset time        88792 non-null  int64   \n",
      " 4   Prior disability level    88792 non-null  int64   \n",
      " 5   Stroke team               88792 non-null  category\n",
      " 6   Use of AF anticoagulents  88792 non-null  int64   \n",
      " 7   Onset-to-arrival time     88792 non-null  float64 \n",
      " 8   Thrombolysis              88792 non-null  int64   \n",
      "dtypes: category(1), float64(3), int64(5)\n",
      "memory usage: 6.3 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "for c in data.columns:\n",
    "    col_type = data[c].dtype\n",
    "    if col_type == 'object' or col_type.name == 'category':\n",
    "        data[c] = data[c].astype('category')\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit lightGBM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide into X (features) and y (labels)\n",
    "\n",
    "We will separate out our features (the data we use to make a prediction) from our label (what we are trying to predict).\n",
    "By convention our features are called `X` (usually upper case to denote multiple features), and the label (thrombolysis or not) `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('Thrombolysis', axis=1)\n",
    "y = data['Thrombolysis']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average thromboylsis (this is the expected outcome of each patient, without knowing anything about the patient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average treatment: 0.3\n"
     ]
    }
   ],
   "source": [
    "print (f'Average treatment: {round(y.mean(),2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Represent hospital ID as an integer-encoded categorical feature\n",
    "\n",
    "Create the LightGBM data containers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the LightGBM data containers\n",
    "categorical_features = [\"Stroke team\"]\n",
    "lgbm_dataset = lightgbm.Dataset(X, label=y, \n",
    "                                categorical_feature=categorical_features, \n",
    "                                free_raw_data=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Light GBM model\n",
    "\n",
    "We will fit a model to all of the data (rather than train/test splits used to assess accuracy).\n",
    "\n",
    "Using https://www.kaggle.com/code/ezietsman/simple-python-lightgbm-example/data to get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#n_estimators is set to a \"large value\". The actual number of trees build will depend on early stopping and 1000 define only the absolute maximum\n",
    "clf = lgb.LGBMClassifier(num_leaves= 15, max_depth=-1, \n",
    "                         random_state=314, \n",
    "                         silent=True, \n",
    "                         metric='None', \n",
    "                         n_jobs=4, \n",
    "                         n_estimators=1000,\n",
    "                         colsample_bytree=0.9,\n",
    "                         subsample=0.9,\n",
    "                         learning_rate=0.1)\n",
    "\n",
    "\n",
    "\n",
    "#force larger number of max trees and smaller learning rate\n",
    "clf.fit(X_train, y_train, **fit_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LGBMClassifier(verbose=-1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier(verbose=-1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LGBMClassifier(verbose=-1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LGBMClassifier(verbose=-1)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the predictions for each patient (in terms of the classification, and the probability of being in either class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X)\n",
    "y_proba = model.predict_proba(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the models accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 0.867\n"
     ]
    }
   ],
   "source": [
    "accuracy = np.mean(y == y_pred)\n",
    "print(f'Model accuracy: {accuracy:0.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get SHAP values\n",
    "\n",
    "TreeExplainer is a fast and exact method to estimate SHAP values for tree models and ensembles of trees.\n",
    "Using this we can calculate the SHAP values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'TXHRP7672C'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m explainer \u001b[38;5;241m=\u001b[39m shap\u001b[38;5;241m.\u001b[39mTreeExplainer(model)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Get SHAP values\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m shap_values \u001b[38;5;241m=\u001b[39m \u001b[43mexplainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m shap_values \u001b[38;5;241m=\u001b[39m explainer\u001b[38;5;241m.\u001b[39mshap_values(X)\n\u001b[1;32m      8\u001b[0m base_value \u001b[38;5;241m=\u001b[39m explainer\u001b[38;5;241m.\u001b[39mexpected_value\n",
      "File \u001b[0;32m~/anaconda3/envs/samuel2/lib/python3.8/site-packages/shap/explainers/_tree.py:217\u001b[0m, in \u001b[0;36mTree.__call__\u001b[0;34m(self, X, y, interactions, check_additivity)\u001b[0m\n\u001b[1;32m    214\u001b[0m     feature_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_feature_names\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m interactions:\n\u001b[0;32m--> 217\u001b[0m     v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshap_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_call\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_additivity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_additivity\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapproximate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapproximate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(v) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    219\u001b[0m         v \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack(v, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# put outputs at the end\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/samuel2/lib/python3.8/site-packages/shap/explainers/_tree.py:349\u001b[0m, in \u001b[0;36mTree.shap_values\u001b[0;34m(self, X, y, tree_limit, approximate, check_additivity, from_call)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmodel_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlightgbm\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m approximate, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapproximate=True is not supported for LightGBM models!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 349\u001b[0m     phi \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moriginal_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_iteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtree_limit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_contrib\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    350\u001b[0m     \u001b[38;5;66;03m# Note: the data must be joined on the last axis\u001b[39;00m\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39moriginal_model\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/samuel2/lib/python3.8/site-packages/lightgbm/basic.py:3538\u001b[0m, in \u001b[0;36mBooster.predict\u001b[0;34m(self, data, start_iteration, num_iteration, raw_score, pred_leaf, pred_contrib, data_has_header, is_reshape, **kwargs)\u001b[0m\n\u001b[1;32m   3536\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3537\u001b[0m         num_iteration \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 3538\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_iteration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_iteration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3539\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mraw_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_leaf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_contrib\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3540\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mdata_has_header\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_reshape\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/samuel2/lib/python3.8/site-packages/lightgbm/basic.py:848\u001b[0m, in \u001b[0;36m_InnerPredictor.predict\u001b[0;34m(self, data, start_iteration, num_iteration, raw_score, pred_leaf, pred_contrib, data_has_header, is_reshape)\u001b[0m\n\u001b[1;32m    846\u001b[0m     preds, nrow \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__pred_for_csc(data, start_iteration, num_iteration, predict_type)\n\u001b[1;32m    847\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m--> 848\u001b[0m     preds, nrow \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pred_for_np2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_iteration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_iteration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredict_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    850\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/samuel2/lib/python3.8/site-packages/lightgbm/basic.py:938\u001b[0m, in \u001b[0;36m_InnerPredictor.__pred_for_np2d\u001b[0;34m(self, mat, start_iteration, num_iteration, predict_type)\u001b[0m\n\u001b[1;32m    936\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m preds, nrow\n\u001b[1;32m    937\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 938\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_iteration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_iteration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredict_type\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/samuel2/lib/python3.8/site-packages/lightgbm/basic.py:900\u001b[0m, in \u001b[0;36m_InnerPredictor.__pred_for_np2d.<locals>.inner_predict\u001b[0;34m(mat, start_iteration, num_iteration, predict_type, preds)\u001b[0m\n\u001b[1;32m    898\u001b[0m     data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(mat\u001b[38;5;241m.\u001b[39mreshape(mat\u001b[38;5;241m.\u001b[39msize), dtype\u001b[38;5;241m=\u001b[39mmat\u001b[38;5;241m.\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    899\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# change non-float data to float data, need to copy\u001b[39;00m\n\u001b[0;32m--> 900\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    901\u001b[0m ptr_data, type_ptr_data, _ \u001b[38;5;241m=\u001b[39m c_float_array(data)\n\u001b[1;32m    902\u001b[0m n_preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_num_preds(start_iteration, num_iteration, mat\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], predict_type)\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'TXHRP7672C'"
     ]
    }
   ],
   "source": [
    "# Set up the method to estimate SHAP values for tree models and ensembles of\n",
    "# trees\n",
    "explainer = shap.TreeExplainer(model)\n",
    "\n",
    "# Get SHAP values\n",
    "# shap_values = explainer(X) gives error about \"ValueError: could not convert string to float: 'TXHRP7672C'\"\n",
    "\n",
    "# Found code that offers this as a solution\n",
    "shap_values = explainer.shap_values(X)\n",
    "base_value = explainer.expected_value\n",
    "\n",
    "# But then need to rebuild the shap_values in the format as would have gotten from \"explainer(X)\" if it worked\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "May not be possible to calc SHAP of light GBM using categorical variables?\n",
    "\n",
    "https://github.com/slundberg/shap/issues/170\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For light GBM the explainer returns the base value which is the same value for all instances [explainer.base_values], and the shap values per feature [explainer.shap_values(X)].\n",
    "\n",
    "Each of these returns a list with two elements - I believe that this is one for each of the target values: 0 (not thrombolysed) and 1 (thrombolysed)\n",
    "\n",
    "There is an array of SHAP values for each instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(shap_values[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With each array containing a vlaue per feature (the hospital ID is in as a single feature, with a single SHAP value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(shap_values[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And one base value for all instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_value[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View SHAP values using beeswarm plot\n",
    "\n",
    "The beeswarm plot gives a good visual representation of the general SHAP value pattern for the whole dataset. \n",
    "\n",
    "Each feature is shown on a separate row. It shows the distribution of the SHAP values for each feature. The colour represents the feature data value, and the shape of the data points represent the distribution of the features SHAP values (with a bulge representing a larger number of points, and a thin row representing fewer points). A SHAP value less than 0 (as seen on the x-axis) contributes to the likelihood that the passenger will not survive, whereas a SHAP value greater than 0 contributes to the likelihood that the passenger will survive.\n",
    "\n",
    "The actual predction of whether a passenger will survive is the sum of each of the SHAP feature values and the SHAP base value.\n",
    "\n",
    "Here we see that the first line on the beeswarm represents the feature male. A red data points represents a high data value (a male passenger), and a blue datapoint represents a low data value (a female passenger). Being male contributes to the likelihood that they will not survive, whereas being female contributes to the likelihood that they will survive. Female passengers can have a stronger contribution to the outcome (up to +4) than compared to the males (down to -2).\n",
    "\n",
    "The third line on the beeswarm represents the feature Age. A red data points represents a high data value (an old passenger), a purple datapoint represents a mid point (a middle aged passenger) and a blue datapoint represents a child. The older the passenger the stronger the contribution to the likelihood that they will not survive, the younger the passenger the stronger the contribution to the likelihood that they will survive. There are more datapoints around the 0 SHAP value (which are coloured purple, and so represent the middle aged passengers) than at the extremes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# None of these work\n",
    "shap.plots.beeswarm(shap_values,X,show=False)\n",
    "#shap.plots.beeswarm(shap_values[1],show=False)\n",
    "#shap.plots.beeswarm(explainer, show=False)\n",
    "#shap.plots.beeswarm(explainer(X), show=False)\n",
    "#shap.plots.beeswarm(explainer.shap_values(X), show=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get SHAP interaction values\n",
    "Use the TreeExplainer to also calculate the SHAP main effect and SHAP interaction values (the sum of which give the SHAP values for each feature)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get SHAP interaction values\n",
    "shap_interaction = explainer.shap_interaction_values(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SHAP interaction values have a matrix of values (per pair of features) per instance.\\\n",
    "In this case, each of the 891 instances has a 4x4 matrix of SHAP interaction values (with the SHAP main effect on the diagonal positions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_interaction.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show SHAP interation matrix (with main effect on the diagonal positions) for the first instance. Notice how the SHAP interation for pairs of features are symmetrical across the diagonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_interaction[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP interaction matrix: show mean absolute values\n",
    "Here we see the absolute mean of the SHAP interaction values for all of the instances.\\\n",
    "The values on the diagonal show the main effect for the feature, and the other values show the SHAP interaction for pairs of features (these are symetrical across the diagonal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_abs_interactions = pd.DataFrame(\n",
    "    np.abs(shap_interaction).mean(axis=(0)),\n",
    "    index=X.columns, columns=X.columns)\n",
    "\n",
    "mean_abs_interactions.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The proportion of SHAP that is from the interactions: calculated from the absolute mean\n",
    "Looking at all of the instances together, what proportion of the SHAP value comes from the SHAP interations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_shap = mean_abs_interactions.sum().sum()\n",
    "\n",
    "interaction_shap = (mean_abs_interactions.sum().sum() - \n",
    "                    np.diagonal(mean_abs_interactions).sum().sum())\n",
    "\n",
    "print(f'The proportion of the SHAP values coming from the interactions are: '\n",
    "      f'{interaction_shap/total_shap:0.3f}')\n",
    "print(f'The proportion of the SHAP values coming from the main effects are: '\n",
    "      f'{1 - (interaction_shap/total_shap):0.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The proportion of SHAP that is from the interactions: calculated per instance from the absolute values\n",
    "Looking at each instances, what proportion of the SHAP value comes from the SHAP interations. Show the range of proportions (one per instance) as a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum the absolute interaction matrix per instance\n",
    "abs_total_shap_per_instance = np.abs(shap_interaction).sum(axis=(1,2))\n",
    "\n",
    "# Initialise list\n",
    "proportion_interaction = []\n",
    "\n",
    "# For each instance\n",
    "for i in range(abs_total_shap_per_instance.shape[0]):\n",
    "    # sum the absolute feature interactions (off diagonal positions)\n",
    "    abs_interaction = (abs_total_shap_per_instance[i] - \n",
    "                       np.diagonal(np.abs(shap_interaction[i])).sum())\n",
    "    # calculate the proportion from feature interactions\n",
    "    proportion_interaction.append(\n",
    "                abs_interaction / abs_total_shap_per_instance[i])\n",
    "\n",
    "# plot as histogram\n",
    "plt.hist(proportion_interaction);\n",
    "plt.xlabel(\"Proportion of SHAP from feature interactions \\n\"\n",
    "           \"(calculated from absolute SHAP values)\")\n",
    "plt.ylabel(\"Number of instances\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP interaction matrix: represented as histograms \n",
    "\n",
    "Show the distribution of all of the instance values for each SHAP interation and SHAP main effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"male\",\"Pclass\",\"Age\",\"SibSp\"]\n",
    "n_features = len(features)\n",
    "\n",
    "# Find the largest value used for the y axis in all of the histograms in the \n",
    "#   subplots (use this to set the max for each subplot)\n",
    "y_max = -1\n",
    "fig, axes = plt.subplots(1)\n",
    "for i in range(n_features):\n",
    "    for j in range(n_features):    \n",
    "        axes.hist(shap_interaction[:,i,j])\n",
    "        ylims = axes.get_ylim()\n",
    "        # Store if greater than found so far\n",
    "        y_max = max(y_max, ylims[1])\n",
    "# Don't display plot\n",
    "plt.close(fig)\n",
    "\n",
    "# Setup figure with subplots\n",
    "fig, axes = plt.subplots(\n",
    "    nrows=len(features), \n",
    "    ncols=len(features))\n",
    "axes = axes.ravel()        \n",
    "\n",
    "count = 0\n",
    "for i in range(n_features):\n",
    "    for j in range(n_features):    \n",
    "        ax=axes[count]\n",
    "        ax.hist(shap_interaction[:,i,j])\n",
    "        ax.set_xlabel(f\"SHAP interaction value from {features[i]}-{features[j]}\")\n",
    "        ax.set_ylabel(\"Number of instances\")\n",
    "        ax.set_ylim(0, y_max)\n",
    "        count += 1\n",
    "        \n",
    "fig.set_figheight(16)\n",
    "fig.set_figwidth(16)\n",
    "plt.tight_layout(pad=2)\n",
    "#fig.subplots_adjust(hspace=0.4, wspace=0.9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show a worked example for the first instance\n",
    "Start with the feature values, and then show the SHAP values and how they can be represented as main effect and interactions. Also show that by summing them along with the base value gives the model output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance = 0\n",
    "target_category = [\"not survive\", \"survive\"]\n",
    "# Show data for first example\n",
    "print ('Showing a worked example for the first instance')\n",
    "print ('==============================================')\n",
    "print ()\n",
    "print ('------------------')\n",
    "print ('Feature data values')\n",
    "print ('------------------')\n",
    "print (X.iloc[instance])\n",
    "\n",
    "# Model output\n",
    "prob_survive = y_proba[instance][1]\n",
    "logodds_survive = np.log(prob_survive/(1 -prob_survive))\n",
    "print ()\n",
    "print ('-------------------')\n",
    "print ('Model output values')\n",
    "print ('-------------------')\n",
    "print (f'1. Model probability [not survive, survive]: ' +\n",
    "       f'{np.round(y_proba[instance],3)}')\n",
    "##print ('------------')\n",
    "#print (np.round(y_proba[instance],3))\n",
    "print (f'\\n2. Model log odds survive: {round(logodds_survive,3)}')\n",
    "#print ('------------')\n",
    "#print(round(logodds_survive,3))\n",
    "cat = np.int(y_pred[instance])\n",
    "print (f'\\n3. Model classification: {cat} ({target_category[cat]})')\n",
    "##print ('------------')\n",
    "#print (y_pred[instance])\n",
    "\n",
    "print ()\n",
    "print ('-----------------')\n",
    "print ('SHAP base value (log odds)')\n",
    "print ('---------------')\n",
    "print (shap_values.base_values[instance])\n",
    "print ('\\nNote: This is the same value for all of the instances. This is the ' +\n",
    "       'models best guess without additional knowledge about the instance')\n",
    " \n",
    "#example_shap = pd.DataFrame(shap_values.values[instance],columns=X.columns)\n",
    "print ()\n",
    "print ('-----------------')\n",
    "print ('SHAP values (log odds)')\n",
    "print ('------------')\n",
    "# print (example_shap)\n",
    "v = shap_values.values[instance][0]\n",
    "print (f'{X.columns.values[0]}: {v:0.3f}')\n",
    "v = shap_values.values[instance][1]\n",
    "print (f'{X.columns.values[1]}: {v:0.3f}')\n",
    "v = shap_values.values[instance][2]\n",
    "print (f'{X.columns.values[2]}: {v:0.3f}')\n",
    "v = shap_values.values[instance][3]\n",
    "print (f'{X.columns.values[3]}: {v:0.3f}')\n",
    "# print (shap_values.values[instance])\n",
    "v = shap_values.values[instance].sum()\n",
    "print (f'Total = {v:0.3f}')\n",
    "\n",
    "print ('\\nNote: These are patient dependent')\n",
    "#print ()\n",
    "#print ('-----------------')\n",
    "#print ('Sum of SHAP values')\n",
    "#print ('------------')\n",
    "#print (shap_values.values[instance].sum())\n",
    "\n",
    "print (f'\\nThe \"Model log odds survive\" value ({logodds_survive:0.3g}, ' +\n",
    "       f'see above) is calculated by adding up the SHAP base value ' +\n",
    "       f'({shap_values.base_values[instance]:0.3f}, see above) with ' +\n",
    "       f'all of the SHAP values for each feature ' +\n",
    "       f'({shap_values.values[instance].sum():0.3f}, see above)')\n",
    "print (f'{shap_values.base_values[instance]:0.3f} + ' +\n",
    "       f'{shap_values.values[instance].sum():0.3f} = ' +\n",
    "       f'{logodds_survive:0.3f}')\n",
    "\n",
    "# SHAP interaction values for first employee\n",
    "example_interaction = pd.DataFrame(shap_interaction[instance],\n",
    "                                   index=X.columns,columns=X.columns)\n",
    "row_total = example_interaction.sum(axis=0)\n",
    "column_total = example_interaction.sum(axis=1)\n",
    "total = example_interaction.sum().sum()\n",
    "example_interaction['Total'] = row_total\n",
    "example_interaction.loc['Total'] = column_total\n",
    "example_interaction.loc['Total']['Total'] = total\n",
    "\n",
    "print ()\n",
    "print ('-----------------')\n",
    "print ('SHAP interactions (log odds)')\n",
    "print ('-----------------')\n",
    "print ('\\n* Each instance has a different SHAP value for the features. This ' +\n",
    "       'is because the model is also capturing the interaction between pairs ' +\n",
    "       'of features, and how that contributes to the features SHAP value.')\n",
    "print ('* Each feature has a SHAP main effect (on the diagonal) and a SHAP ' +\n",
    "       'interaction effect with each of the other features (off the diagonal)')\n",
    "print ('* SHAP interaction is split symetrically, eg. age-male is the same ' +\n",
    "       'as male-age.')\n",
    "print ('* For each feature, the sum of the SHAP main effect and all of its ' +\n",
    "       'SHAP interaction values = SHAP value for the feature (shown in ' +\n",
    "       '\"Total\", and can be compared to the SHAP values above)')\n",
    "print ()\n",
    "print (example_interaction)\n",
    "\n",
    "print ('------------------')\n",
    "print ('\\nThe model prediction for each instance can be arrived at by ' +\n",
    "       'starting at the SHAP base value, and adding on the SHAP values from ' +\n",
    "       'all of the the main effects (one per feature) and from all of the ' +\n",
    "       'SHAP interactions (two per pair of features).')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sum of the SHAP value components (base + main effects + interactions) = model prediction \n",
    "We've seen a worked through example for one instance that the sum of the SHAP interactions and main effects and base value equals the model output (the log odds of predicted P). \n",
    "\n",
    "Here we show that it holds for all of the instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model output: probability survive\n",
    "prob_survive = y_proba[:,1]\n",
    "# Calculate log odds\n",
    "logodds_survive = np.log(prob_survive/(1 -prob_survive))\n",
    "\n",
    "# sum each matrix to get a value per instance\n",
    "total_shap_per_instance = shap_values.base_values + shap_interaction.sum(axis=(1,2))\n",
    "\n",
    "x = total_shap_per_instance\n",
    "y = logodds_survive\n",
    "\n",
    "# Fit a regression line to the points\n",
    "slope, intercept, r_value, p_value, std_err = \\\n",
    "    stats.linregress(x, y)\n",
    "r_square = r_value ** 2\n",
    "y_pred = intercept + (x * slope)\n",
    "\n",
    "# Create scatter plot with regression line\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.scatter(x, y)\n",
    "plt.plot (x, y_pred, color = 'red')\n",
    "text = f'R squared: {r_square:.3f}\\np: {p_value:0.4f}'\n",
    "ax1.text(2, -5, text, \n",
    "         bbox=dict(facecolor='white', edgecolor='black'))\n",
    "ax1.set_xlabel(\"Sum of the SHAP main effects and SHAP interations and SHAP base value\")\n",
    "ax1.set_ylabel(\"Model output (log odds of survival)\")\n",
    "plt.title(\"For each instance, check get same value from two sources\")\n",
    "plt.grid()\n",
    "#plt.savefig('./output/scatter_plot_hosp_shap_vs_10k_thrombolysis.jpg', dpi=300,\n",
    "#    bbox_inches='tight', pad_inches=0.2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Histogram of total SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(total_shap_per_instance)\n",
    "plt.xlabel(\"Total SHAP value\")\n",
    "plt.ylabel(\"Number of instances\")\n",
    "plt.title(\"Count of instances with total SHAP values (base + main effects + feature interactions)\")\n",
    "plt.grid()\n",
    "#plt.savefig('./output/scatter_plot_hosp_shap_vs_10k_thrombolysis.jpg', dpi=300,\n",
    "#    bbox_inches='tight', pad_inches=0.2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How the SHAP main effect (or interaction) varies across the instances: using violin plots\n",
    "\n",
    "### SHAP main effect of male-male\n",
    "\n",
    "For this example lets focus on the feature \"male\". This feature has two possible values: males (1) and females (0).\n",
    "\n",
    "From the histogram in the matrix showing the main effect for the feature male, we can see that there are ~550 instances with a male main effect of about -1, and ~300 instances with a male main effect of about 2.\n",
    "\n",
    "From this we can not see which of these instances are which gender (male or female).\n",
    "\n",
    "Here we will plot this same data using a violin plot, a violin for each gender.\n",
    "\n",
    "We can see from the violin plot that the main effect (male-male) is quite different depending on whether the instance is male (a negative SHAP main effect value) or female (a positive SHAP main effect value).\n",
    "\n",
    "This means that the feature will contribute a strong likelihood of survival if the instance is female, and a mid-strong likelihood of not surviving if the instance is male. This matches the story that we took from the beeswarm plot of the SHAP values, however as we have now extracted just the main effect the violin plot is showing a distinct effect for gender. That means that the points on the beeswarm that join up these two distinct groups are from the feature interactions - they \"muddy\" the relationship (blur the edges, say) between the feature value and SHAP value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_violin_shap_interaction(X, shap_interaction, main_feature, \n",
    "                                 interaction_feature):\n",
    "    \"\"\"\n",
    "    Given the two features (main_feature and interaction_feature), plot the SHAP \n",
    "    interations as violin plots. \n",
    "    The main_feature will have it's data values displayed on the x axis. \n",
    "    The interaction_feature determines the SHAP interaction values that are \n",
    "    displayed in the violins.\n",
    "    If the same feature name is in both (main_feature and interaction_feature)\n",
    "    then the main effect will be displayed.\n",
    "    \n",
    "    X [pandas dataframe]: Feature per column, instance per row\n",
    "    shap_interaction [3D numpy array]: [instance][feature][feature]\n",
    "    main_feature [string]: feature name\n",
    "    interaction_feature [string]: feature name\n",
    "    \n",
    "    \"\"\"\n",
    "    # Get the unqiue categories for the main feature\n",
    "    category_list = list(X[main_feature].unique())\n",
    "\n",
    "    # Setup dictionary and keys (key for each category, each key will hold a \n",
    "    #   list of SHAP interaction values for that category)\n",
    "    shap_interaction_by_category_dict = {}\n",
    "    for i in category_list:\n",
    "        shap_interaction_by_category_dict[i]=[]\n",
    "    \n",
    "    # Store number of instances and number of categories\n",
    "    n_instances = X.shape[0]\n",
    "    n_categories = len(category_list)\n",
    "    \n",
    "    # For each instance put its instance interaction value in the corresponding \n",
    "    #   list (based on the instances category for the main feature)\n",
    "    for i in range(n_instances):\n",
    "        # Identify the instances category for the main feature\n",
    "        category = X.iloc[i][main_feature]\n",
    "\n",
    "        # Get the SHAP interaction value for the instance\n",
    "        instance_interaction = pd.DataFrame(\n",
    "            shap_interaction[i],index=X.columns,columns=X.columns)\n",
    "        \n",
    "        # Get the feature pairing interaction value\n",
    "        value = instance_interaction.loc[main_feature][interaction_feature]\n",
    "\n",
    "        # Store value in the dictionary using category as the key\n",
    "        shap_interaction_by_category_dict[category].append(value)\n",
    "    \n",
    "    # Set violin width relative to count of instances\n",
    "    width = [(len(shap_interaction_by_category_dict[category])/n_instances) \n",
    "             for category in category_list]\n",
    "\n",
    "    # Create list of series to use in violin plot (one per violin)\n",
    "    shap_per_category = [pd.Series(shap_interaction_by_category_dict[category]) \n",
    "                         for category in category_list]\n",
    "    \n",
    "    # create violin plot\n",
    "    fig = plt.figure(figsize=(5,5))\n",
    "    ax = fig.add_subplot()\n",
    "    ax.violinplot(shap_per_category, showmedians=True, widths=width, )\n",
    "\n",
    "    # customise the axes\n",
    "    ax.set_title(\"\")\n",
    "    ax.get_xaxis().set_tick_params(direction='out')\n",
    "    ax.xaxis.set_ticks_position('bottom')\n",
    "    ax.set_xticks(np.arange(1, len(category_list) + 1))\n",
    "    ax.set_xticklabels(category_list, rotation=90, fontsize=12)\n",
    "    ax.set_xlim(0.25, len(category_list) + 0.75)\n",
    "    ax.set_ylabel(\n",
    "        f'SHAP interaction value for {main_feature}-{interaction_feature}',\n",
    "        fontsize=12)\n",
    "    ax.set_xlabel(f'Feature: {main_feature}', fontsize=12)\n",
    "\n",
    "    plt.subplots_adjust(bottom=0.15, wspace=0.05)\n",
    "\n",
    "    # Add line at Shap = 0\n",
    "    ax.plot([0, n_categories + 1], [0,0],c='0.5')\n",
    "    \n",
    "    return()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_violin_shap_interaction(X, shap_interaction, \"male\", \"male\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP interaction of male-Pclass\n",
    "\n",
    "We can also see the range of the SHAP interaction value between the features male-Pclass (divided by the male categories).\n",
    "\n",
    "This shows that for females the SHAP interaction value between male-Pclass ranges from -0.6 to 0.9, and for males it has a smaller range (-0.6 to 0.3). Since this is in addition to the main effect (for which all females had a strong likelihood to survive), for some females their likelihood for surviving is further increased, whereas for others their likelihood for surviving is reduced - but never enough to have a likelihood of not surviving. Remember that we'd need to add on all of the other SHAP interations to get the likelihood of surviving for females."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_violin_shap_interaction(X, shap_interaction, \"male\", \"Pclass\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Violin plots can only display the values for one of the features in a feature-pairing - by it's placement on the x axis. In the violin plot above we can only see the value for the feature male, but not the value for the feature Pclass.\n",
    "\n",
    "This can be solved by using a SHAP dependency plot - they can show the values for both features and the SHAP interaction value. This is shown in the following section, and we will introduce them using the same data as used in these two violin plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How the SHAP main effect (or interaction) varies across the instances: using dependence plots\n",
    "\n",
    "### SHAP main effect of male-male\n",
    "\n",
    "We can see from the violin plot that the main effect (male-male) is quite different depending on whether the instance is male (a negative SHAP main effect value) or female (a positive SHAP main effect value).\n",
    "\n",
    "This means that the feature will contribute a strong likelihood of survival if the instance is female, and a mid-strong likelihood of not surviving if the instance is male.\n",
    "\n",
    "A dependence plot of the same data that's in the violin plot will represent it as individual points, instead of as a distribution. Doing so, it will plot all of the points on two points on the x axis: 0 for female, and 1 for male. A lot of information is lost due to overlap. To see more detail we add some jitter to the x-axis to spread the points out and so we cna get a sense of the density of the points in relation to the y value.\n",
    "\n",
    "Here we see the same information as in the violin plot: the main effect (male-male) is quite different depending on whether the instance is male (a negative SHAP main effect value) or female (a positive SHAP main effect value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "shap.dependence_plot(\n",
    "    (\"male\", \"male\"),\n",
    "    shap_interaction, X,\n",
    "    display_features=X,\n",
    "    x_jitter=0.5,\n",
    "    ax=ax,\n",
    "    show=False)\n",
    "\n",
    "# Add line at Shap = 0\n",
    "n_violins = X[\"male\"].nunique()\n",
    "ax.plot([-1, n_violins], [0,0],c='0.5') \n",
    "\n",
    "fig.set_figheight(5)\n",
    "fig.set_figwidth(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP interaction of male-Pclass\n",
    "\n",
    "A hinderence of using a violin plot to show the data for the SHAP interaction of a feature pair is that we can only show one of the feature values (on the x axis). \n",
    "\n",
    "When using a dependence plot to show the data for the SHAP interaction of a feature pairing we can display both of the feature values: the point location on the x axis shows the value of one of the features, and the colour of the point shows the value of the other feature.\n",
    "\n",
    "Note: The SHAP interaction for feature pairings (e.g. male-Pclass) is splt between male-Pclass and Pclass-male. The total SHAP interaction is therefore 2* the individual interactions. We could multiply the interation by 2 to get the full SHAP interaction. Here we will plot both permutations and acknowledge that it's the sum of the pair (LHS shows columns as male and colour as Pclass. RHS shows columns as Pclass and colour as male). Each graph contains the exact same data points, and it is possible to match up the identical block of data points across the graphs. For example the purple points in the LHS graph represent the Pclass 2, and we can see that for these points they have a positive SHAP interaction value for female (x-axis 0) and negative SHAP interaction value for male (x-axis 1). We can see these two blocks of purple points in the RHS graph, with both blocks now aligned on the x-axis with value 2, and now coloured blue for female (with positive SHAP interaction value) or red for male (with negative SHAP interaction value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(121)\n",
    "shap.dependence_plot(\n",
    "    (\"male\", \"Pclass\"),\n",
    "    shap_interaction, X,\n",
    "    display_features=X,\n",
    "    x_jitter=0.5,\n",
    "    ax=ax,\n",
    "    show=False)\n",
    "\n",
    "# Add line at Shap = 0\n",
    "n_violins = X[\"male\"].nunique()\n",
    "ax.plot([-1, n_violins + 1], [0,0],c='0.5') \n",
    "\n",
    "ax1 = fig.add_subplot(122)\n",
    "shap.dependence_plot(\n",
    "    (\"Pclass\", \"male\"),\n",
    "    shap_interaction, X,\n",
    "    display_features=X,\n",
    "    x_jitter=0.5,\n",
    "    ax=ax1,\n",
    "    show=False)\n",
    "\n",
    "# Add line at Shap = 0\n",
    "n_classes = X[\"Pclass\"].nunique()\n",
    "ax1.plot([-1, n_classes + 1], [0,0],c='0.5') \n",
    "\n",
    "fig.set_figheight(5)\n",
    "fig.set_figwidth(15)\n",
    "fig.subplots_adjust(wspace=.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid of SHAP dependence plots\n",
    "\n",
    "We will now show all of the SHAP interaction values in a grid of plots: each row and column represents a feature.\n",
    "\n",
    "The diagonal graphs show the SHAP main effect for each feature. The SHAP interactions between features are off the diagonal, these are split symetrically (eg. age-male == male-age).\n",
    "\n",
    "The SHAP main effect for feature male is shown in the top left (position [0, 0]). As already discussed, this shows that when the feature value is female, this has a strong contribution to the models prediction that the passenger will survive. And when this feature value is male there is a mid-strong contribution that the passenger will not survive.\n",
    "\n",
    "The plot in position [1,1] shows the SHAP main effect for class. This shows that first class contributes a strong likelihood to survive, second class does not have much contribution, and third class contributes a strong likelihood not to survive.\n",
    "\n",
    "But on top of these main effects we can see the contributon from the interation of these features. This is shown in positions [0, 1] and [1, 0]. The SHAP interaction between male and Pclass, and Pclass and male.\n",
    "\n",
    "Graph in grid position [0, 1] (first row, second column) shows the SHAP interaction between male and Pclass, the data has been split into columns by the value of the gender feature (female on left, male on right), and the colour represents the class feature (first class = blue, second class = purple, third class = red). The value represents the contribution to the likelihood of this passenger surviving due to this combination of values - this is in addition to the main effect that we saw in the top left.\n",
    "\n",
    "It can be seen that passengers in first or second class further increase the likelihood of survival for females, and not surviving for males, as the SHAP interation value is in the same sign to the SHAP main effect: A female passenger in first or second class will increase the likelihood of survival from the models prediction, and so will further help your survival in addition to the fact that you are female (as we saw in the SHAP main effect); similarly a male passenger in first or second class will increase the likelihood of not surviving, and so will further contribute to the likelihood that you will not survive, in addition to the fact that you are male (as we saw in the SHAP main effect).\n",
    "However the converse is true for passengers in third class, as the SHAP interaction value is in the opposite sign to the SHAP main effect. A female passenger in third class will have a negative contribution to the survival (but remember that the main effect for female is a strong likelihood to survive), and if you are male in third class this combination will have a positive contribution to your survival (but remember that the main effect for male is a mid-strong likelihood to not survive).\n",
    "\n",
    "The grid of dependency graphs are a mirror image across the diagonal. Meaning that the same data is shown in position [0,1] as in [1,0] just with the feature being displayed in the column or by colour is switched over.\n",
    "\n",
    "Looking at the graph in position [1, 0] (second row, first column) shows the identical SHAP interation values for the features male - PClass, as we have just discussed above. Now the columns are per class (first, second, third) and the colour is by gender (male, female). Here we see that for first and second class females contributes that there is a mid likelihood to not survive, whereas if male then contributes a positive likelihood to survive. But that this is opposite for third class, where is it the females (red) with a positive likelihood to survive. This is also on top of the main effect from Pclass.\n",
    "\n",
    "Resources used to make the grid of dependence plots: https://stackoverflow.com/questions/58510005/python-shap-package-how-to-plot-a-grid-of-dependence-plots \\\n",
    "(for future reference, but not yet used here: https://gist.github.com/eddjberry/3c1818a780d3cb17390744d6e215ba4d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"male\",\"Pclass\",\"Age\",\"SibSp\"]\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    nrows=len(features), \n",
    "    ncols=len(features))\n",
    "axes = axes.ravel()\n",
    "\n",
    "count = 0\n",
    "for f1 in features:\n",
    "    for f2 in features:\n",
    "        shap.dependence_plot(\n",
    "            (f1, f2), shap_interaction, X, x_jitter=0.5, display_features=X,\n",
    "            show=False, ax=axes[count])\n",
    "        # Add line at Shap = 0\n",
    "        n_classes = X[f1].nunique()\n",
    "        axes[count].plot([-1, n_classes], [0,0],c='0.5')   \n",
    "        count += 1\n",
    "        \n",
    "fig.set_figheight(16)\n",
    "fig.set_figwidth(20)\n",
    "#plt.tight_layout(pad=2)\n",
    "fig.subplots_adjust(hspace=0.4, wspace=0.9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the individual instance values you can unpick and understand how each instance gets their classification.\n",
    "\n",
    "Each instance is represented in the grid of SHAP depencency plots, and so this shows all of the relationships that the model uses to derive it's predictions for the whole dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other SHAP plotting options\n",
    "\n",
    "The SHAP library also offers other plotting options, such as a summary plot based on the beeswarm plot. \n",
    "\n",
    "We will show it here for completeness, however we found it to be tricky to interprete, and left gaps in our understanding of the relationships (it left us with further questions). \n",
    "\n",
    "It was due to this that we created our grid of dependency plots (as displayed above)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP interactions summary plot (a grid of beeswarms)\n",
    "\n",
    "The beeswarm plot above showed the overall SHAP value for the feature. This next plot (a grid of beeswarms) shows the SHAP main effect and SHAP interactions for each feature. Each row and column represents a feature. The beeswarms on the diagonal represent the SHAP main effect for that feature, and those off the diagonal represent the SHAP interations with the other features.\n",
    "\n",
    "\n",
    "The graphs are symmetrical around the diagonal, and so the shape of the data in the corresponding graph about the diagonal are the same, however the points are coloured based on the value of the feature represented by the row. For example, the first row this showing the feature male, so red represents the value male, and blue represents the value female. The second row shows the feature Pclass where blue represents first class, purple represents second class, and red represents third class. The third row shows the feature Age where blue represents the youngest, purple represent middle aged and red represents oldest. The fourth row shows the feature SibSp where blue represents no siblings, purple represents 3-4 siblings, and red represents seven siblings.\n",
    "\n",
    "The shape of the data is based on the density of points that have the SHAP interaction value as displayed on the x axis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display summary plot\n",
    "shap.summary_plot(shap_interaction, X,show=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* SHAP interactions are awesome! \n",
    "* Viewing them as a grid of SHAP dependency plots clearly shows the overall relationships that the model uses to derive it's predictions for the whole dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
